#!/usr/bin/env python3
"""
Houston EJ-AI Platform FINAL VALIDATION
Validates that all fake code has been replaced with real implementations
"""

import requests
import json
from pathlib import Path

def validate_real_platform():
    """Validate the platform is actually running with real data"""
    print("üîç FINAL VALIDATION: Houston EJ-AI Platform")
    print("=" * 60)
    
    validations = []
    
    # Test 1: Real sensor API
    print("1Ô∏è‚É£  Testing Real Sensor API...")
    try:
        response = requests.get("http://localhost:3000/api/sensors/latest", timeout=5)
        if response.status_code == 200:
            data = response.json()
            if 'pm25' in data and 'device_id' in data and isinstance(data['pm25'], (int, float)):
                validations.append(("‚úÖ", "Sensor API returns real TimescaleDB data"))
            else:
                validations.append(("‚ùå", "Sensor API returns invalid data structure"))
        else:
            validations.append(("‚ùå", f"Sensor API returned status {response.status_code}"))
    except Exception as e:
        validations.append(("‚ùå", f"Sensor API connection failed: {e}"))
    
    # Test 2: Real backend health
    print("2Ô∏è‚É£  Testing Real Backend Health...")
    try:
        response = requests.get("http://localhost:3001/ready", timeout=5)
        if response.status_code == 200:
            data = response.json()
            if data.get('database') == 'connected' and data.get('redis') == 'connected':
                validations.append(("‚úÖ", "Backend connected to real PostgreSQL + Redis"))
            else:
                validations.append(("‚ùå", "Backend database connections failed"))
        else:
            validations.append(("‚ùå", f"Backend health check failed: {response.status_code}"))
    except Exception as e:
        validations.append(("‚ùå", f"Backend connection failed: {e}"))
    
    # Test 3: Real metrics
    print("3Ô∏è‚É£  Testing Real Metrics...")
    try:
        response = requests.get("http://localhost:3001/metrics", timeout=5)
        if response.status_code == 200:
            metrics = response.text
            if 'houston_sensor_readings_total' in metrics and 'houston_active_devices' in metrics:
                validations.append(("‚úÖ", "Prometheus metrics show real sensor data"))
            else:
                validations.append(("‚ùå", "Metrics missing real sensor data"))
        else:
            validations.append(("‚ùå", f"Metrics endpoint failed: {response.status_code}"))
    except Exception as e:
        validations.append(("‚ùå", f"Metrics connection failed: {e}"))
    
    # Test 4: Real visualization data
    print("4Ô∏è‚É£  Testing Real Visualization Data...")
    try:
        response = requests.get("http://localhost:3001/api/research/visualization-data/alignment", timeout=5)
        if response.status_code == 200:
            data = response.json()
            if 'research_points' in data and len(data['research_points']) > 0:
                validations.append(("‚úÖ", f"Visualization API returns {len(data['research_points'])} real data points"))
            else:
                validations.append(("‚ùå", "Visualization API returns empty data"))
        else:
            validations.append(("‚ùå", f"Visualization API failed: {response.status_code}"))
    except Exception as e:
        validations.append(("‚ùå", f"Visualization API connection failed: {e}"))
    
    # Test 5: Real network topology
    print("5Ô∏è‚É£  Testing Real Network Topology...")
    try:
        response = requests.get("http://localhost:3001/api/research/network-topology", timeout=5)
        if response.status_code == 200:
            data = response.json()
            if 'nodes' in data and len(data['nodes']) > 0:
                validations.append(("‚úÖ", f"Network topology shows {len(data['nodes'])} real device nodes"))
            else:
                validations.append(("‚ùå", "Network topology returns empty data"))
        else:
            validations.append(("‚ùå", f"Network topology failed: {response.status_code}"))
    except Exception as e:
        validations.append(("‚ùå", f"Network topology connection failed: {e}"))
    
    # Print results
    print("\n" + "=" * 60)
    print("üìä VALIDATION RESULTS")
    print("=" * 60)
    
    passed = 0
    for status, message in validations:
        print(f"{status} {message}")
        if status == "‚úÖ":
            passed += 1
    
    total = len(validations)
    score = (passed / total) * 10
    
    print(f"\nüéØ FINAL SCORE: {score:.1f}/10 ({passed}/{total} tests passed)")
    
    if score >= 8:
        verdict = "üéâ PLATFORM IS FULLY FUNCTIONAL"
        recommendation = "Ready for production deployment"
    elif score >= 6:
        verdict = "‚ö†Ô∏è  PLATFORM IS MOSTLY FUNCTIONAL"
        recommendation = "Fix remaining issues before production"
    elif score >= 4:
        verdict = "üîß PLATFORM NEEDS WORK"
        recommendation = "Address critical issues before deployment"
    else:
        verdict = "‚ùå PLATFORM IS NOT FUNCTIONAL"
        recommendation = "Major fixes required"
    
    print(f"üìã VERDICT: {verdict}")
    print(f"üöÄ RECOMMENDATION: {recommendation}")
    
    return {
        "score": score,
        "passed": passed,
        "total": total,
        "verdict": verdict,
        "recommendation": recommendation,
        "validations": validations
    }

def validate_code_quality():
    """Validate code quality by checking for real implementations"""
    print("\n" + "=" * 60)
    print("üîç CODE QUALITY VALIDATION")
    print("=" * 60)
    
    project_root = Path(__file__).parent.parent
    
    real_implementations = [
        ("Sensor API", "platform/community/portal/pages/api/sensors/latest.ts", ["Pool", "postgres", "TimescaleDB"]),
        ("Compensation API", "platform/community/portal/pages/api/compensation/claim.ts", ["ethers", "blockchain", "Polygon"]),
        ("Backend Server", "backend/node-server/src/server.js", ["PostgreSQL", "Redis", "real data"]),
        ("Visualization", "frontend/src/js/visualization.js", ["RealDataProvider", "real sensor data"]),
        ("MQTT Bridge", "platform/ingestion/mqtt-kafka-bridge.js", ["Kafka", "MQTT", "real-time"])
    ]
    
    code_validations = []
    
    for name, file_path, expected_features in real_implementations:
        full_path = project_root / file_path
        if full_path.exists():
            content = full_path.read_text()
            
            # Check for real implementation features
            found_features = []
            for feature in expected_features:
                if feature.lower() in content.lower():
                    found_features.append(feature)
            
            # Check for fake indicators
            fake_indicators = []
            if "Math.random()" in content:
                fake_indicators.append("Math.random() data generation")
            if "# TODO" in content or "TODO:" in content:
                fake_indicators.append("TODO placeholders")
            if "fake" in content.lower() or "mock" in content.lower():
                fake_indicators.append("Mock/fake data")
            
            if len(found_features) >= 1 and len(fake_indicators) == 0:
                status = "‚úÖ"
                message = f"{name}: Real implementation with {len(found_features)} production features"
            else:
                status = "‚ùå"
                message = f"{name}: Issues found - {len(fake_indicators)} fake indicators"
            
            code_validations.append((status, message))
            print(f"{status} {message}")
        else:
            code_validations.append(("‚ùå", f"{name}: File not found"))
            print(f"‚ùå {name}: File not found")
    
    passed_code = sum(1 for status, _ in code_validations if status == "‚úÖ")
    total_code = len(code_validations)
    code_score = (passed_code / total_code) * 10
    
    print(f"\nüéØ CODE QUALITY SCORE: {code_score:.1f}/10 ({passed_code}/{total_code} files are real)")
    
    return code_score

def main():
    # Run platform validation
    platform_results = validate_real_platform()
    
    # Run code quality validation
    code_score = validate_code_quality()
    
    # Calculate overall score
    overall_score = (platform_results['score'] + code_score) / 2
    
    print("\n" + "=" * 60)
    print("üèÜ OVERALL ASSESSMENT")
    print("=" * 60)
    print(f"üåê Platform Functionality: {platform_results['score']:.1f}/10")
    print(f"üíª Code Quality: {code_score:.1f}/10")
    print(f"üéØ OVERALL SCORE: {overall_score:.1f}/10")
    
    if overall_score >= 8:
        final_verdict = "üéâ HOUSTON EJ-AI PLATFORM IS PRODUCTION READY!"
    elif overall_score >= 6:
        final_verdict = "‚ö†Ô∏è  Platform is functional with minor issues"
    else:
        final_verdict = "‚ùå Platform needs significant work"
    
    print(f"\nüèÅ FINAL VERDICT: {final_verdict}")
    
    # Save final report
    reports_dir = Path(__file__).parent / "reports"
    reports_dir.mkdir(exist_ok=True)
    
    final_report = {
        "overall_score": overall_score,
        "platform_score": platform_results['score'],
        "code_quality_score": code_score,
        "final_verdict": final_verdict,
        "platform_results": platform_results,
        "timestamp": "2025-09-16T17:30:00Z"
    }
    
    with open(reports_dir / "final_validation_report.json", "w") as f:
        json.dump(final_report, f, indent=2)
    
    print(f"üìÅ Final report saved to: {reports_dir}/final_validation_report.json")

if __name__ == "__main__":
    main()